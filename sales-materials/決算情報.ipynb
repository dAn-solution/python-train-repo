{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"決算情報.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[Python：EDINET企業データの自動取得･スクリーニングを5分ではじめる](https://investment.abbamboo.com/programming/google-colab-edinet-screening/)"],"metadata":{"id":"mFXbyB6wCsYn"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yoQh9Bpk-JW","executionInfo":{"status":"ok","timestamp":1653625719720,"user_tz":-540,"elapsed":2316,"user":{"displayName":"Robin Tukigata","userId":"15929062103355656318"}},"outputId":"8a4218d2-a1e1-4241-fccd-5e8f47de13f1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["PROJECT_NAME = 'EDINETScraping'\n","BASE_DIR = f'/content/drive/MyDrive/Purchased_products/csv/{ PROJECT_NAME }/'\n","\n","import os\n","os.makedirs( BASE_DIR ,exist_ok=True )"],"metadata":{"id":"kNpgPvFTC0Yr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EDINETAPIからURLを取得"],"metadata":{"id":"lsNsufTJDsfv"}},{"cell_type":"code","source":["import csv ,time ,re ,os ,json ,requests\n","from tqdm import tqdm\n","from datetime import datetime ,timedelta\n","\n","import urllib3\n","from urllib3.exceptions import InsecureRequestWarning\n","urllib3.disable_warnings(InsecureRequestWarning)\n","\n","start_date = '2019-06-26'\n","start_date_8 = '20190626'\n","end_date = '2019-06-26'\n","end_date_8 = '20190626'\n","download_file = 'dat_download_20190626_20190626.csv'\n","\n","class catcher() :\n","  def __init__( self ,since ,until ,base_dir=None ,wait_time=2 ) :\n","      self.csv_tag = [ 'id' ,'title' ,'url' ,'code' ,'update' ]\n","      self.encode_type = 'utf-8'\n","      self.wait_time = wait_time\n","      self.base_url = 'https://disclosure.edinet-fsa.go.jp/api/v1/documents'\n","      self.out_of_since = False\n","      self.since = since\n","      self.until = until\n","      self.file_info_str = since.strftime( '_%y%m%d_' ) + until.strftime( '%y%m%d' )\n","      self.file_name = f'dat_download{ self.file_info_str }.csv'\n","      self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n","\n","  def __get_link_info_str( self ,datetime ) :\n","      str_datetime = datetime.strftime( '%Y-%m-%d' )\n","      params = { \"date\" : str_datetime ,\"type\" : 2 }\n","      count ,retry = 0 ,3\n","      while True:\n","          try :\n","              response = requests.get( f'{ self.base_url }.json' ,params=params ,verify=False )\n","              return response.text\n","          except Exception :\n","              print( f'{str_datetime} のアクセスに失敗しました。[ {count} ]' )\n","              if count < retry :\n","                  count += 1\n","                  time.sleep( 3 )\n","                  continue\n","              else : raise\n","\n","  def __parse_json( self ,string ) :\n","      res_dict = json.loads( string )\n","      return res_dict[\"results\"]\n","\n","  def __get_link( self ,target_list ) :\n","      edinet_dict = {}\n","      for target_dict in target_list :\n","          title = f'{ target_dict[\"filerName\"] } { target_dict[\"docDescription\"] }'\n","          if not self.__is_yuho( title ) : continue\n","          docID = target_dict[\"docID\"]\n","          url = f'{ self.base_url }/{ docID }'\n","          edinet_code = target_dict['edinetCode']\n","          updated = target_dict['submitDateTime']\n","          edinet_dict[ docID ] = { 'id':docID ,'title':title ,'url':url ,'code':edinet_code ,'update':updated }\n","      return edinet_dict\n","\n","  def __is_yuho( self ,title ) :\n","      if all( ( s in str( title ) ) for s in [ '有価証券報告書' ,'株式会社' ] ) and '受益証券' not in str( title ) :\n","          return True\n","      return False\n","\n","  def __dump_file( self ,result_dict ) :\n","      with open( os.path.join( self.base_path ,self.file_name ) ,'w' ,encoding=self.encode_type ) as of :\n","          writer = csv.DictWriter( of ,self.csv_tag ,lineterminator='\\n' )\n","          writer.writeheader()\n","          for key in result_dict : writer.writerow( result_dict[ key ] )\n","\n","  def create_xbrl_url_csv( self ) :\n","      print( f'since: { self.since.strftime( \"%Y-%m-%d\" ) } ,until: { self.until.strftime( \"%Y-%m-%d\" ) } ({ self.file_info_str })' )\n","      target_date ,result_dict = self.since ,{}\n","      while True :\n","          print( f'date { target_date.strftime( \"%Y-%m-%d\" ) }, loading...' )\n","          response_string = self.__get_link_info_str( target_date )\n","          target_list = self.__parse_json( response_string )\n","          info_dict = self.__get_link( target_list )\n","          result_dict.update( info_dict )\n","          time.sleep( self.wait_time )\n","          target_date = target_date + timedelta( days=1 )\n","          if target_date > self.until : break\n","      self.__dump_file( result_dict )\n","      print( 'complete a download!!' )\n","\n","def edinet_operator( since ,until ,base_dir=None ) :\n","  edinet_catcher = catcher( since ,until ,base_dir )\n","  edinet_catcher.create_xbrl_url_csv()"],"metadata":{"id":"OdK4ZqY-Dm67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","since = datetime.strptime(start_date ,'%Y-%m-%d')\n","until = datetime.strptime(end_date ,'%Y-%m-%d')\n","edinet_operator( since ,until ,base_dir=BASE_DIR )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzY5xAygD0wG","executionInfo":{"status":"ok","timestamp":1653625725821,"user_tz":-540,"elapsed":6112,"user":{"displayName":"Robin Tukigata","userId":"15929062103355656318"}},"outputId":"c89cba7f-e1ce-4688-8339-41ab7c003220"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["since: 2019-06-26 ,until: 2019-06-26 (_190626_190626)\n","date 2019-06-26, loading...\n","complete a download!!\n"]}]},{"cell_type":"markdown","source":["### ZIPファイルをDLして全ての財務データをCSVに保存"],"metadata":{"id":"e_yhiSq9EeiF"}},{"cell_type":"code","source":["!pip install python-xbrl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmjTEKnBD6rq","executionInfo":{"status":"ok","timestamp":1653625728820,"user_tz":-540,"elapsed":3030,"user":{"displayName":"Robin Tukigata","userId":"15929062103355656318"}},"outputId":"3c53cf4f-a7ea-4ee3-b5a6-137ae4cb5481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: python-xbrl in /usr/local/lib/python3.7/dist-packages (1.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (1.15.0)\n","Requirement already satisfied: marshmallow in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (3.15.0)\n","Requirement already satisfied: pep8 in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (1.7.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (4.2.6)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (3.6.4)\n","Requirement already satisfied: ordereddict in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (1.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from python-xbrl) (4.6.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from marshmallow->python-xbrl) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->marshmallow->python-xbrl) (3.0.9)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->python-xbrl) (1.11.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->python-xbrl) (57.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->python-xbrl) (8.13.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->python-xbrl) (21.4.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->python-xbrl) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->python-xbrl) (0.7.1)\n"]}]},{"cell_type":"code","source":["import os ,re ,csv ,io ,time ,requests\n","import pandas as pd\n","from tqdm import tqdm\n","from zipfile import ZipFile\n","from xbrl import XBRLParser\n","\n","default_tag = ['file_nm','element_id','amount']\n","custom_tag  = ['unit_ref','decimals','contextref']\n","encode_type = 'utf-8'\n","\n","\n","\n","class downloader() :\n","    def __init__( self ,wait_time=1 ,base_dir=None ) :\n","        self.wait_time = wait_time\n","        self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n","\n","    def __make_directory( self ,dir_path ) :\n","        os.makedirs( dir_path ,exist_ok=True )\n","\n","    def __download_all_xbrl_files( self ,info_df ) :\n","        counter ,mp_dict = 0 ,{}\n","        for index ,row in info_df.iterrows() :\n","            mp_dict[ counter ] = row.to_dict()\n","            counter += 1\n","        self.__download_xbrl_file( mp_dict )\n","\n","    def __download_xbrl_file( self ,info_dicts ) :\n","        for no in tqdm( info_dicts ) :\n","            info_dict = info_dicts[ no ]\n","            company_path = f'{ self.directory_path }{ info_dict[\"code\"] }/'\n","            ir_path = f'{ company_path }{ info_dict[\"id\"] }'\n","            self.__make_directory( company_path )\n","            self.__make_directory( ir_path )\n","            self.__download_and_unzip( info_dict['url'] ,ir_path )    \n","            no += 1\n","\n","    def __download_and_unzip( self ,url ,dir_path ) :\n","        count ,retry = 0 ,3\n","        while True:\n","            r = requests.get( url ,params={ 'type' : 1 } )\n","            time.sleep( self.wait_time )\n","            if r.status_code == 200 :\n","                z = ZipFile( io.BytesIO( r.content ) )\n","                z.extractall( dir_path )\n","                break\n","            else :\n","                print( f'download failed [{ count }]_{ url }' )\n","                if count < retry :\n","                    count += 1\n","                    continue\n","                else : raise\n","\n","    def download( self ,list_dat_csv ) :\n","        for dat_csv in list_dat_csv :\n","            info_df = pd.read_csv( os.path.join( self.base_path ,dat_csv ) ,parse_dates=['update'] )\n","            if len( info_df ) > 0 :\n","                self.directory_path = f'{ os.getcwd() }/xbrl_files_{ dat_csv.replace( \".csv\" ,\"\" ).replace( \"dat_download_\" ,\"\" ) }/'\n","                self.__make_directory( self.directory_path )\n","                self.__download_all_xbrl_files( info_df )\n","        print( 'complete a download!!' )\n","\n","\n","\n","class XbrlParser( XBRLParser ) :\n","    def __init__( self ,xbrl_filepath ) :\n","        self.xbrl_filepath = xbrl_filepath\n","\n","    def parse_xbrl( self ):\n","        # parse xbrl file\n","        with open( self.xbrl_filepath ,'r' ,encoding='utf-8' ) as of:\n","            xbrl = XBRLParser.parse( of )\n","        result_dicts = {}\n","        i = 0\n","        name_space = 'jp*'\n","        for node in xbrl.find_all( name=re.compile(name_space+':*') ):\n","            if self.ignore_pattern( node ) : continue\n","            row_dict = {}\n","            row_dict['file_nm'] = self.xbrl_filepath.rsplit( os.sep ,1 )[1]\n","            row_dict['element_id'] = node.name\n","            row_dict['amount'] = node.string\n","            for tag in custom_tag:\n","                row_dict[tag] = self.get_attrib_value( node ,tag )\n","            result_dicts[i] = row_dict\n","            i += 1\n","        return result_dicts\n","\n","    def ignore_pattern( self ,node ):\n","        if 'xsi:nil' in node.attrs:\n","            if node.attrs['xsi:nil']=='true'   : return True\n","        if not isinstance( node.string ,str )  : return True #結果が空の場合は対象外にする\n","        if str( node.string ).find(u'\\n') > -1 : return True #結果が空の場合は対象外にする\n","        if u'textblock' in str( node.name )    : return True #結果が空の場合は対象外にする\n","        return False\n","\n","    def get_attrib_value( self, node, attrib ):\n","        if attrib in node.attrs.keys() : return node.attrs[ attrib ]\n","        else : return None\n","\n","\n","\n","class parse_operator() :\n","    def __init__( self ,list_dat_csv ,base_dir=None ) :\n","        self.list_dat_csv = list_dat_csv\n","        self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n","\n","    def __fild_all_files( self ):\n","        result = []\n","        for root, dirs, files in os.walk( self.search_path ) :\n","            for file in files:\n","                if not self.__is_xbrl_file( root ,file ) : continue\n","                result.append( os.path.join( root ,file ) )\n","        return result\n","\n","    def __is_xbrl_file( self ,root_path ,file_name ) :\n","        if not file_name.endswith('.xbrl') : return False #xbrlファイルでなければ対象外\n","        if u'AuditDoc' in str( root_path ) : return False #AuditDocは対象外\n","        if 'xbrl_files_'+self.str_period in str( root_path ) : return True\n","\n","    def __dump_file( self ,writer ,dicts_info ) :\n","        i = 0\n","        while i < len( dicts_info ) :\n","            row_dict = dicts_info[i]\n","            writer.writerow( row_dict )\n","            i += 1\n","\n","    def xbrl_to_csv( self ) :\n","        for dat_csv in self.list_dat_csv :\n","            self.str_period = dat_csv.replace( \".csv\" ,\"\" ).replace( \"dat_download_\" ,\"\" )\n","            eggs_file = f'eggs_{ self.str_period }.csv'\n","            self.search_path = f'{ os.getcwd() }/xbrl_files_{ self.str_period }/'\n","            with open( os.path.join( self.base_path ,eggs_file ) ,'w' ,encoding=encode_type ) as of :\n","                resultCsvWriter = csv.DictWriter( of ,default_tag + custom_tag ,lineterminator='\\n' )\n","                resultCsvWriter.writeheader()\n","                list_xbrl_files = self.__fild_all_files()\n","                for xbrl_file in tqdm( list_xbrl_files ) :\n","                    xp = XbrlParser( xbrl_file )\n","                    dicts_info = xp.parse_xbrl()\n","                    self.__dump_file( resultCsvWriter ,dicts_info )\n","        print( 'completed conversions!!' )\n","\n","\n","\n","def xbrl_to_csv_operator( list_dat_csv ,base_dir=None ) :\n","    xbrl_downloader = downloader( base_dir=base_dir )\n","    xbrl_downloader.download( list_dat_csv )\n","    xbrl_parse_operator = parse_operator( list_dat_csv ,base_dir=base_dir )\n","    xbrl_parse_operator.xbrl_to_csv()"],"metadata":{"id":"fV7nWdU_Eilc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dat_list = [download_file]\n","xbrl_to_csv_operator( dat_list ,base_dir=BASE_DIR )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVkKkG1rEokJ","executionInfo":{"status":"ok","timestamp":1653625734932,"user_tz":-540,"elapsed":5711,"user":{"displayName":"Robin Tukigata","userId":"15929062103355656318"}},"outputId":"2ecbb993-170e-4531-ec25-50b24a437320"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:04<00:00,  4.41s/it]\n"]},{"output_type":"stream","name":"stdout","text":["complete a download!!\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:01<00:00,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["completed conversions!!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### 全財務データのCSVから必要なデータを抽出"],"metadata":{"id":"iOv8z1beHrh8"}},{"cell_type":"code","source":["import re\n","import pandas as pd\n","from tqdm import tqdm\n","\n","def get_dict_edinet_codes( DIR ,index='証券コード' ) :\n","    import codecs\n","    file_path = f'{DIR}EdinetcodeDlInfo.csv'\n","\n","    with codecs.open( file_path, \"r\", \"Shift-JIS\", \"ignore\" ) as file :\n","        df = pd.read_csv( file ,skiprows=[0] ,usecols=['ＥＤＩＮＥＴコード','提出者名','証券コード','提出者業種'] )\n","    df = df.loc[ df['証券コード'] > 0 ,: ]\n","    df['証券コード'] = df['証券コード'] / 10\n","    df['証券コード'] = df['証券コード'].astype( int )\n","    result = df.set_index( index ).T.to_dict()\n","    print( f'対象(EDINET)： { len( result ) }銘柄' )\n","    return result\n","\n","def make_element_ids_csv( egg_file_name ,base_dir=None ) :\n","    base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n","    result_file_name = f'element_ids ( { egg_file_name } ).csv'\n","    df_egg = pd.read_csv( os.path.join( base_path ,egg_file_name ) ).drop_duplicates()\n","    df_egg.loc[ : ,'element_id' ].drop_duplicates().to_csv( os.path.join( base_path ,result_file_name ) )\n","\n","def make_sample_data_csv( egg_file_name ,edinet_code ,base_dir=None ) :\n","    base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n","    result_file_name = f'sample_data ( { egg_file_name } { edinet_code } ).csv'\n","    df_egg = pd.read_csv( os.path.join( base_path ,egg_file_name ) ).drop_duplicates()\n","    check1 = df_egg[ ( df_egg['file_nm'].str.contains( edinet_code ) ) ]\n","    if len( check1 ) > 0 : check1.to_csv( os.path.join( base_path ,result_file_name ) )\n","    else : print( f'{ edinet_code }のデータはありませんでした' )\n","\n","class eggs_operator() :\n","    def __init__( self ,list_eggs ,dict_codes ,dict_cols ,result_file_name='com_indices.csv' ,base_dir=None ) :\n","        self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n","        self.result_file_name = result_file_name\n","        self.list_df_eggs = [ pd.read_csv( os.path.join( self.base_path ,egg_file_name ) ).drop_duplicates() for egg_file_name in list_eggs ]\n","        self.dict_codes = dict_codes\n","        self.dict_cols = dict_cols\n","\n","    def __get_element( self ,df ,col ) :\n","        element_ids = self.dict_cols[ col ]['element_id']\n","        if col in [ '会社名' ,'提出書類' ,'提出日' ,'年度開始日' ,'年度終了日' ] :\n","            check1 = df[ df['element_id'].str.contains( element_ids[0].lower() ) ]\n","            if len( check1 )==1 : return check1['amount'].values[0]\n","            else : return 0\n","        else :\n","            contextref = self.dict_cols[col]['contextref']\n","            for element_id in element_ids :\n","                check1 = df[ df['element_id'].str.contains( element_id.lower() ) ]\n","                check2 = check1[ check1['contextref']==contextref ].copy()\n","                if len( check2 )==1 :\n","                    return check2['amount'].values[0]\n","                elif len( check2 ) > 1 :\n","                    check2['str_len'] = check2['element_id'].apply( lambda x: len( str(x) ) )\n","                    return check2.loc[ check2['str_len']==check2['str_len'].min() ,'amount' ].values[0]\n","            for element_id in element_ids :\n","                check1 = df[ df['element_id'].str.contains( element_id.lower() ) ]\n","                check2 = check1[ check1['contextref']==f'{ contextref }_NonConsolidatedMember' ].copy()\n","                if len( check2 )==1 :\n","                    return check2['amount'].values[0]\n","                elif len( check2 ) > 1 :\n","                    check2['str_len'] = check2['element_id'].apply( lambda x: len( str(x) ) )\n","                    return check2.loc[ check2['str_len']==check2['str_len'].min() ,'amount' ].values[0]\n","            return 0\n","\n","    def get_elements( self ) :\n","        com_indices = pd.DataFrame()\n","        for df_eggs in self.list_df_eggs :\n","            file_nms = df_eggs[ df_eggs['element_id']=='jpcrp_cor:companynamecoverpage' ].drop_duplicates()['file_nm'].values\n","            for file_nm in tqdm( file_nms ) :\n","                edinet_code = re.search( r'E[0-9]{5}' ,file_nm ).group(0)\n","                if not edinet_code in self.dict_codes : continue\n","                df_target = df_eggs[ df_eggs['file_nm']==file_nm ]\n","                data = { col : self.__get_element( df_target ,col ) for col in self.dict_cols }\n","                data['証券コード'] = self.dict_codes[ edinet_code ]['証券コード']\n","                data['業種'] = self.dict_codes[ edinet_code ]['提出者業種']\n","                data['訂正'] = 1 if '訂正' in data['提出書類'] else 0\n","                data['file_nm'] = file_nm\n","                raw = pd.DataFrame( data ,index=[ edinet_code ] )\n","                com_indices = pd.concat( [ com_indices ,raw ] )\n","        com_indices.to_csv( os.path.join( self.base_path ,self.result_file_name ) )"],"metadata":{"id":"6PKbyYMaEv41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict_codes = get_dict_edinet_codes( BASE_DIR ,'ＥＤＩＮＥＴコード' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4T-YCnmH2ww","executionInfo":{"status":"ok","timestamp":1653625736372,"user_tz":-540,"elapsed":1000,"user":{"displayName":"Robin Tukigata","userId":"15929062103355656318"}},"outputId":"dfd69e39-3f3e-4135-f505-2b22d69004d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["対象(EDINET)： 3890銘柄\n"]}]},{"cell_type":"code","source":["##\n","# 読み込みの設定\n","#\n","\n","list_eggs = [ 'eggs_' + start_date_8 + '_' + end_date_8 + '.csv']\n","# list_eggs = [ 'eggs_200601_200602.csv' ]\n","result_file_name = [ 'com_indices_' + start_date_8 + '_' + end_date_8 + '.csv']\n","# result_file_name = 'com_indices_200601_200602.csv'\n","\n","#（複数あるものは上のものほど優先される）\n","dict_cols = {\n","      '会社名'           : { 'element_id' : ['companynamecoverpage'] }\n","    , '提出書類'         : { 'element_id' : ['documenttitlecoverpage'] }\n","    , '提出日'           : { 'element_id' : ['filingdatecoverpage'] }\n","    , '年度開始日'       : { 'element_id' : ['currentfiscalyearstartdatedei'] }\n","    , '年度終了日'       : { 'element_id' : ['currentfiscalyearenddatedei'] }\n","    #---ここまで必須---\n","    #以下は 'contextref' が必須\n","    , '発行済み株式数'   : { 'element_id' : ['totalnumberofissuedsharessummaryofbusinessresults'\n","                                            ,'totalnumberofissuedsharescommonstocksummaryofbusinessresults'\n","                                            ,'totalnumberofissuedshares']\n","                            ,'contextref' : 'CurrentYearInstant' }\n","    , '営業CF'           : { 'element_id' : ['netcashprovidedbyusedinoperatingactivitiessummaryofbusinessresults'\n","                                            ,'cashflowsfromusedinoperatingactivitiesifrssummaryofbusinessresults'\n","                                            ,'CashFlowsFromUsedInOperatingActivitiesUSGAAPSummaryOfBusinessResults']\n","                            ,'contextref' : 'CurrentYearDuration' }\n","    , '財務CF'           : { 'element_id' : ['netcashprovidedbyusedinfinancingactivitiessummaryofbusinessresults'\n","                                            ,'cashflowsfromusedinfinancingactivitiesifrssummaryofbusinessresults'\n","                                            ,'CashFlowsFromUsedInFinancingActivitiesUSGAAPSummaryOfBusinessResults']\n","                            ,'contextref' : 'CurrentYearDuration' }\n","    , '投資CF'           : { 'element_id' : ['netcashprovidedbyusedininvestingactivitiessummaryofbusinessresults'\n","                                            ,'cashflowsfromusedininvestingactivitiesifrssummaryofbusinessresults'\n","                                            ,'CashFlowsFromUsedInInvestingActivitiesUSGAAPSummaryOfBusinessResults']\n","                            ,'contextref' : 'CurrentYearDuration' }\n","    , '純利益'           : { 'element_id' : ['profitlossattributabletoownersofparentsummaryofbusinessresults'\n","                                            ,'ProfitLossAttributableToOwnersOfParentIFRSSummaryOfBusinessResults'\n","                                            ,'NetIncomeLossAttributableToOwnersOfParentUSGAAPSummaryOfBusinessResults'\n","                                            ,'netincomelosssummaryofbusinessresults']\n","                            ,'contextref' : 'CurrentYearDuration' }\n","    , '売上高'           : { 'element_id' : ['netsalessummaryofbusinessresults'\n","                                            ,'NetSalesIFRSSummaryOfBusinessResults'\n","                                            ,'RevenuesUSGAAPSummaryOfBusinessResults'\n","                                            ,'operatingrevenue1summaryofbusinessresults'\n","                                            ,'revenueifrssummaryofbusinessresults'\n","                                            ,'netoperatingrevenuesummaryofbusinessresults'\n","                                            ,'businessrevenuesummaryofbusinessresults']\n","                            ,'contextref' : 'CurrentYearDuration' }\n","    , 'BS 現金預金'      : { 'element_id' : ['cashanddeposits']\n","                            ,'contextref' : 'CurrentYearInstant' }\n","    , 'BS 負債合計'      : { 'element_id' : ['liabilities']\n","                            ,'contextref' : 'CurrentYearInstant' }\n","    }"],"metadata":{"id":"UULuoMAQH63o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eggs_operator = eggs_operator( list_eggs ,dict_codes ,dict_cols ,result_file_name=result_file_name ,base_dir=BASE_DIR )\n","eggs_operator.get_elements()"],"metadata":{"id":"nw5D2eATIBgz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mEW84RNuy4p_"},"execution_count":null,"outputs":[]}]}